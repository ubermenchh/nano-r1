{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/umangkaushik/qwen2-5-3b-openmath-grpo?scriptVersionId=224006891\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Training a CoT Model using Qwen2.5, GRPO and Unsloth","metadata":{}},{"cell_type":"markdown","source":"## Installing the Dependencies","metadata":{}},{"cell_type":"markdown","source":"For Kaggle","metadata":{}},{"cell_type":"code","source":"!rm -rf *","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture \n!pip install -q unsloth vllm \n# Temporarily install a specific TRL nightly version \n!pip install -q git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b \n!pip install -q triton==3.1.0 \n!pip install -qU pynvml \n!pip install -q math-verify[antlr4_13_2] ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For Google-Colab","metadata":{}},{"cell_type":"code","source":"# %%capture\n# # Skip restarting message in Colab\n# import sys; modules = list(sys.modules.keys())\n# for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n\n# !pip install unsloth vllm\n# !pip install --upgrade pillow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel, PatchFastRL \nPatchFastRL(\"GRPO\", FastLanguageModel) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:47:46.055815Z","iopub.execute_input":"2025-02-23T05:47:46.056081Z","iopub.status.idle":"2025-02-23T05:48:17.769653Z","shell.execute_reply.started":"2025-02-23T05:47:46.056061Z","shell.execute_reply":"2025-02-23T05:48:17.768865Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Patching Xformers to fix some performance issues.\nðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from unsloth import is_bfloat16_supported\nimport torch\n\nmax_seq_length = 1024\nlora_rank = 64\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name             = \"Qwen/Qwen2.5-3B-Instruct\",\n    max_seq_length         = max_seq_length,\n    load_in_4bit           = True,\n    fast_inference         = True,\n    max_lora_rank          = lora_rank,\n    gpu_memory_utilization = 0.6\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r                          = lora_rank,\n    target_modules             = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha                 = lora_rank,\n    use_gradient_checkpointing = \"unsloth\",\n    random_state               = 1337\n)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nimport re\n\nSYSTEM_PROMPT = \"\"\"\nRespond in the following format:\n<think>\n...\n</think>\n<answer>\n...\n</answer>\n\"\"\"\n\ndef extract_reasoning_generation(text: str) -> str:\n    if \"<think>\" not in text or \"</think>\" not in text:\n        return \"\"\n    think_part = text.split(\"<think>\")[-1]\n    think_part = think_part.split(\"</think>\")[0]\n    return think_part.strip()\n\ndef get_openr1_math_220k(split: str) -> Dataset:\n    data = load_dataset(\"open-r1/OpenR1-Math-220k\", split=split)\n\n    def transform_record(x):\n        problem_text = x.get(\"problem\", \"\")\n        reasoning_text = extract_reasoning_generation(x.get(\"generations\", \"\")[-1])\n        final_answer = x.get(\"solution\", \"\")\n\n        xml_output = f\"<think>{reasoning_text}</think>\\n<answer>\\n{final_answer}\\n</answer>\"\n\n        return {\n            \"prompt\": [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": problem_text},\n            ],\n            \"solution\": xml_output\n        }\n\n    data = data.map(transform_record).remove_columns(\"messages\")\n    return data\n\ndataset = get_openr1_math_220k(\"train\")\nprint(dataset[0][\"prompt\"])\nprint(dataset[0][\"solution\"])","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_xml_answer(text: str) -> str:\n    \"\"\"\n    Extracts the <answer>...</answer> from the text, ignoring any <reasoning> blocks.\n    \"\"\"\n    if \"<answer>\" not in text or \"</answer>\" not in text:\n        return \"\"\n    answer_part = text.split(\"<answer>\")[-1]\n    answer_part = answer_part.split(\"</answer>\")[0]\n    return answer_part.strip() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_xml_answer(dataset[0][\"solution\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport math\nimport re\nfrom typing import Dict\n\nfrom latex2sympy2_extended import NormalizationConfig\nfrom math_verify import LatexExtractionConfig, parse, verify\n\ndef accuracy_reward(completions, solution, **kwargs):\n    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n    contents = [completion[-1][\"content\"] for completion in completions]\n    rewards = []\n    for content, sol in zip(contents, solution):\n        gold_parsed = parse(\n            extract_xml_answer(sol),\n            extraction_mode=\"first_match\",\n            extraction_config=[LatexExtractionConfig(\n                normalization_config=NormalizationConfig(\n                            nits=False,\n                            malformed_operators=False,\n                            basic_latex=True,\n                            equations=True,\n                            boxed=\"all\",\n                            units=True,\n                        ),\n                        # Ensures that boxed is tried first\n                        boxed_match_priority=0,\n                        try_extract_without_anchor=False,\n            )],\n        )\n        if len(gold_parsed) != 0:\n            # We require the answer to be provided in correct latex (no malformed operators)\n            answer_parsed = parse(content, extraction_config=[LatexExtractionConfig(\n                normalization_config=NormalizationConfig(\n                            nits=False,\n                            malformed_operators=False,\n                            basic_latex=True,\n                            equations=True,\n                            boxed=\"all\",\n                            units=True,\n                        ),\n                        # Ensures that boxed is tried first\n                        boxed_match_priority=0,\n                        try_extract_without_anchor=False,\n            )], extraction_mode=\"first_match\")\n            # Reward 1 if the content is the same as the ground truth, 0 otherwise \n            reward = float(verify(answer_parsed, gold_parsed))\n        else:\n            # If the gold solution is not parseable, we reward 1 to skip this example\n            reward = 1.0\n            # print(\"accuracy_reward: Failed to parse gold solution: \", sol)\n        rewards.append(reward)\n\n    return rewards\n\ndef format_reward(completions, **kwargs):\n    \"\"\"Reward function that checks if the reasoning process is enclosed within <think> and </think> tags, while the final answer is enclosed within <answer> and </answer> tags.\"\"\"\n    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n    completion_contents = [completion[-1][\"content\"] for completion in completions]\n    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE) for content in completion_contents]\n    return [1.0 if match else 0.0 for match in matches]\n\ndef reasoning_steps_reward(completions, **kwargs):\n    r\"\"\"Reward function that checks for clear step-by-step reasoning.\n    Regex pattern:\n        Step \\d+: - matches \"Step 1:\", \"Step 2:\", etc.\n        ^\\d+\\. - matches numbered lists like \"1.\", \"2.\", etc. at start of line\n        \\n- - matches bullet points with hyphens\n        \\n\\* - matches bullet points with asterisks\n        First,|Second,|Next,|Finally, - matches transition words\n    \"\"\"\n    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n    completion_contents = [completion[-1][\"content\"] for completion in completions]\n    matches = [len(re.findall(pattern, content)) for content in completion_contents]\n\n    # Magic nubmer 3 to encourage 3 steps and more, otherwise partial reward\n    return [min(1.0, count / 3) for count in matches]\n\ndef len_reward(completions: list[Dict[str, str]], solutions: list[str], **kwargs) -> float:\n    \"\"\"Compute length-based rewards to discourage overthinking and promote token efficiency.\n\n    Taken from from the Kimi 1.5 tech report: https://arxiv.org/abs/2501.12599\n\n    Args:\n        completions: List of model completions\n        solutions: List of ground truth solutions\n\n    Returns:\n        List of rewards where:\n        - For correct answers: reward = 0.5 - (len - min_len)/(max_len - min_len)\n        - For incorrect answers: reward = min(0, 0.5 - (len - min_len)/(max_len - min_len))\n    \"\"\"\n    contents = [completion[-1][\"content\"] for completion in completions]\n    # First check correctness of answers\n    correctness = []\n    for content, sol in zip(contents, solutions):\n        gold_parsed = parse(extract_xml_answer(sol), extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig(\n            normalization_config=NormalizationConfig(\n                            nits=False,\n                            malformed_operators=False,\n                            basic_latex=True,\n                            equations=True,\n                            boxed=\"all\",\n                            units=True,\n                        ),\n                        # Ensures that boxed is tried first\n                        boxed_match_priority=0,\n                        try_extract_without_anchor=False,\n        )])\n        if len(gold_parsed) == 0:\n            # Skip unparseable examples\n            correctness.append(True)  # Treat as correct to avoid penalizing\n            # print(\"len_reward: Failed to parse gold solution: \", sol)\n            continue\n\n        answer_parsed = parse(content, extraction_config=[LatexExtractionConfig()], extraction_mode=\"first_match\",)\n        correctness.append(verify(answer_parsed, gold_parsed))\n\n    # Calculate lengths\n    lengths = [len(content) for content in contents]\n    min_len = min(lengths)\n    max_len = max(lengths)\n\n    # If all responses have the same length, return zero rewards\n    if max_len == min_len:\n        return [0.0] * len(completions)\n\n    rewards = []\n    for length, is_correct in zip(lengths, correctness):\n        lambda_val = 0.5 - (length - min_len) / (max_len - min_len)\n\n        if is_correct:\n            reward = lambda_val\n        else:\n            reward = min(0, lambda_val)\n\n        rewards.append(float(reward))\n\n    return rewards\n\ndef get_cosine_scaled_reward(\n    min_value_wrong: float = -1.0,\n    max_value_wrong: float = -0.5,\n    min_value_correct: float = 0.5,\n    max_value_correct: float = 1.0,\n    max_len: int = 1000,\n):\n    def cosine_scaled_reward(completions, solution, **kwargs):\n        \"\"\"Reward function that scales based on completion length using a cosine schedule.\n\n        Shorter correct solutions are rewarded more than longer ones.\n        Longer incorrect solutions are penalized less than shorter ones.\n\n        Args:\n            completions: List of model completions\n            solution: List of ground truth solutions\n\n        This function is parameterized by the following arguments:\n            min_value_wrong: Minimum reward for wrong answers\n            max_value_wrong: Maximum reward for wrong answers\n            min_value_correct: Minimum reward for correct answers\n            max_value_correct: Maximum reward for correct answers\n            max_len: Maximum length for scaling\n        \"\"\"\n        contents = [completion[-1][\"content\"] for completion in completions]\n        rewards = []\n\n        for content, sol in zip(contents, solution):\n            gold_parsed = parse(extract_xml_answer(sol), extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig(\n                normalization_config=NormalizationConfig(\n                            nits=False,\n                            malformed_operators=False,\n                            basic_latex=True,\n                            equations=True,\n                            boxed=\"all\",\n                            units=True,\n                        ),\n                        # Ensures that boxed is tried first\n                        boxed_match_priority=0,\n                        try_extract_without_anchor=False,\n            )])\n            if len(gold_parsed) == 0:\n                rewards.append(1.0)  # Skip unparseable examples\n                # print(\"cosine_scaled_reward: Failed to parse gold solution: \", sol)\n                continue\n\n            answer_parsed = parse(content, extraction_config=[LatexExtractionConfig(\n                normalization_config=NormalizationConfig(\n                            nits=False,\n                            malformed_operators=False,\n                            basic_latex=True,\n                            equations=True,\n                            boxed=\"all\",\n                            units=True,\n                        ),\n                        # Ensures that boxed is tried first\n                        boxed_match_priority=0,\n                        try_extract_without_anchor=False,\n            )], extraction_mode=\"first_match\",)\n\n            is_correct = verify(answer_parsed, gold_parsed)\n            gen_len = len(content)\n\n            # Apply cosine scaling based on length\n            progress = gen_len / max_len\n            cosine = math.cos(progress * math.pi)\n\n            if is_correct:\n                min_value = min_value_correct\n                max_value = max_value_correct\n            else:\n                # Swap min/max for incorrect answers\n                min_value = max_value_wrong\n                max_value = min_value_wrong\n\n            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)\n            rewards.append(float(reward))\n\n        return rewards\n\n    return cosine_scaled_reward\n\ndef get_repetition_penalty_reward(ngram_size: int, max_penalty: float):\n    \"\"\"\n    Computes N-gram repetition penalty as described in Appendix C.2 of https://arxiv.org/abs/2502.03373.\n    Reference implementation from: https://github.com/eddycmu/demystify-long-cot/blob/release/openrlhf/openrlhf/reward/repetition.py\n\n    Args:\n    ngram_size: size of the n-grams\n    max_penalty: Maximum (negative) penalty for wrong answers\n    \"\"\"\n    if max_penalty > 0:\n        raise ValueError(f\"max_penalty {max_penalty} should not be positive\")\n\n    def zipngram(text: str, ngram_size: int):\n        words = text.lower().split()\n        return zip(*[words[i:] for i in range(ngram_size)])\n\n    def repetition_penalty_reward(completions, **kwargs) -> float:\n        \"\"\"\n        reward function the penalizes repetitions\n        ref implementation: https://github.com/eddycmu/demystify-long-cot/blob/release/openrlhf/openrlhf/reward/repetition.py\n\n        Args:\n            completions: List of model completions\n        \"\"\"\n\n        contents = [completion[-1][\"content\"] for completion in completions]\n        rewards = []\n        for completion in contents:\n            if completion == \"\":\n                rewards.append(0.0)\n                continue\n            if len(completion.split()) < ngram_size:\n                rewards.append(0.0)\n                continue\n\n            ngrams = set()\n            total = 0\n            for ng in zipngram(completion, ngram_size):\n                ngrams.add(ng)\n                total += 1\n\n            scaling = 1 - len(ngrams) / total\n            reward = scaling * max_penalty\n            rewards.append(reward)\n        return rewards\n\n    return repetition_penalty_reward","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient \nuser_secrets = UserSecretsClient() \nhf_token = user_secrets.get_secret(\"HF_TOKEN\") \n\nimport os\nos.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"wandb_key\")\n\nfrom huggingface_hub import login\nlogin(token=hf_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import GRPOConfig, GRPOTrainer\n\ntraining_args = GRPOConfig(\n    use_vllm = True,\n    learning_rate = 5e-6,\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.99,\n    weight_decay = 0.1,\n    warmup_ratio = 0.1,\n    lr_scheduler_type = \"cosine\",\n    optim = \"adamw_8bit\",\n    logging_steps = 1,\n    bf16 = is_bfloat16_supported(),\n    fp16 = not is_bfloat16_supported(),\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 1,\n    num_generations = 5,\n    max_prompt_length = 2048,\n    max_completion_length = 2048,\n    num_train_epochs = 1,\n    max_steps = 250,\n    save_steps = 250,\n    max_grad_norm = 0.1,\n    report_to = \"wandb\",\n    output_dir = \"qwen2.5-3B-openr1-math\",\n    remove_unused_columns=False\n) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = GRPOTrainer(\n    model = model,\n    processing_class = tokenizer,\n    reward_funcs = [\n        accuracy_reward,\n        format_reward,\n        reasoning_steps_reward,\n        get_cosine_scaled_reward(\n            min_value_wrong=-1.0,\n            max_value_wrong=-0.5,\n            min_value_correct=0.5,\n            max_value_correct=1.0,\n            max_len=1000,\n        ),\n        get_repetition_penalty_reward(ngram_size=3, max_penalty=-0.5),\n    ],\n    args = training_args,\n    train_dataset = dataset\n) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train() \n\nmodel.push_to_hub(\"ubermenchh/Qwen2.5-3B-openr1-math\")\ntokenizer.push_to_hub(\"ubermenchh/Qwen2.5-3B-openr1-math\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"problem = \"\"\"Given that $a > b > 1$ and $Î¸ \\in (0, \\frac{Ï€}{2})$, determine the correct option among the following: A: $a^{\\sin Î¸} < b^{\\sin Î¸}$ B: $ab^{\\sin Î¸} < ba^{\\sin Î¸}$ C: $a\\log _{b}\\sin Î¸ < b\\log _{a}\\sin Î¸$ D: $\\log _{a}\\sin Î¸ < \\log _{b}\\sin Î¸$\"\"\"\ntext = tokenizer.apply_chat_template([\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\" : \"user\", \"content\" : problem},\n], tokenize = False, add_generation_prompt = True)\n\nfrom vllm import SamplingParams\nsampling_params = SamplingParams(\n    temperature = 0.8,\n    top_p = 0.95,\n    max_tokens = 1024,\n)\noutput = model.fast_generate(\n    [text],\n    sampling_params = sampling_params,\n    lora_request = None,\n)[0].outputs[0].text\n\nprint(output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_lora(\"qwen2.5-3B-openr1-math-lora\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"problem = \"\"\"Given that $a > b > 1$ and $Î¸ \\in (0, \\frac{Ï€}{2})$, determine the correct option among the following: A: $a^{\\sin Î¸} < b^{\\sin Î¸}$ B: $ab^{\\sin Î¸} < ba^{\\sin Î¸}$ C: $a\\log _{b}\\sin Î¸ < b\\log _{a}\\sin Î¸$ D: $\\log _{a}\\sin Î¸ < \\log _{b}\\sin Î¸$\"\"\"\ntext = tokenizer.apply_chat_template([\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\" : \"user\", \"content\" : problem},\n], tokenize = False, add_generation_prompt = True)\n\nfrom vllm import SamplingParams\nsampling_params = SamplingParams(\n    temperature = 0.8,\n    top_p = 0.95,\n    max_tokens = 1024,\n)\noutput = model.fast_generate(\n    [text],\n    sampling_params = sampling_params,\n    lora_request = model.load_lora(\"qwen2.5-3B-openr1-math-lora\"),\n)[0].outputs[0].text\n\nprint(output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub_merged(\"ubermenchh/Qwen2.5-3B-open-r1-math\", tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub_merged(\"ubermenchh/Qwen2.5-3B-open-r1-math-lora\", tokenizer, save_method=\"lora\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\nfrom transformers import TextStreamer\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"ubermenchh/Qwen2.5-3B-open-r1-math-lora\",\n    max_seq_length = 1024,\n    dtype = torch.bfloat16,\n    load_in_4bit = True,\n)\nFastLanguageModel.for_inference(model) \n\nSYSTEM_PROMPT = \"\"\"\nRespond in the following format:\n<think>\n...\n</think>\n<answer>\n...\n</answer>\n\"\"\"\n\ntest_question = \"\"\"\nLet $z \\in \\mathbf{C}$, satisfying the condition $a z^{n}+b \\mathrm{i} z^{n-1}+b \\mathrm{i} z-a=0, a, b \\in \\mathbf{R}, m \\in$ $\\mathbf{N}$, find $|z|$.\n\"\"\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": test_question},\n]\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 2048, pad_token_id = tokenizer.eos_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:50:21.384789Z","iopub.execute_input":"2025-02-23T05:50:21.385124Z","iopub.status.idle":"2025-02-23T05:51:28.193441Z","shell.execute_reply.started":"2025-02-23T05:50:21.385099Z","shell.execute_reply":"2025-02-23T05:51:28.192703Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"Device does not support bfloat16. Will change to float16.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.36G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c6f2f2138744475b4fa4c4a130859ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ea7694584494342ac4be9334576d43f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e4a737d5f84d7ab027b5f2a669ff89"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.2.15 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"<think>\nTo solve for \\( |z| \\) given the equation \\( a z^n + b i z^{n-1} + b i z - a = 0 \\), we start by factoring out \\( z \\) from the terms involving \\( z \\):\n\n\\[ a z (z^{n-1} + i z^{n-2} + i z - \\frac{a}{z}) = 0. \\]\n\nThis equation gives us two potential solutions: either \\( z = 0 \\) or the polynomial inside the parentheses must be zero. However, if \\( z = 0 \\), then substituting into the original equation would give \\( a \\cdot 0 + b i \\cdot 0 + b i \\cdot 0 - a = -a \\neq 0 \\) unless \\( a = 0 \\). Since \\( a \\) and \\( b \\) are real numbers and the problem specifies that \\( z \\in \\mathbb{C} \\), we can assume \\( a \\neq 0 \\). Therefore, we focus on solving:\n\n\\[ z^{n-1} + i z^{n-2} + i z - \\frac{a}{z} = 0. \\]\n\nMultiplying through by \\( z \\) to clear the fraction, we get:\n\n\\[ z^n + i z^2 + i z^2 - a = 0 \\]\n\\[ z^n + i z^2 - a = 0. \\]\n\nWe need to find the magnitude of \\( z \\). Let \\( z = re^{i\\theta} \\), where \\( r = |z| \\) and \\( \\theta \\) is the argument of \\( z \\). Then, \\( z^n = r^n e^{in\\theta} \\) and \\( z^{n-1} = r^{n-1} e^{i(n-1)\\theta} \\). Substituting these into the equation, we get:\n\n\\[ r^n e^{in\\theta} + i r^{n-1} e^{i(n-1)\\theta} + i r^{n-1} e^{i(n-1)\\theta} - a = 0. \\]\n\nFor this equation to hold, the magnitudes and phases must match. Noting that the term \\( i r^{n-1} e^{i(n-1)\\theta} + i r^{n-1} e^{i(n-1)\\theta} \\) will have a phase that depends on \\( n-1 \\), we see that the only way for the equation to hold for all \\( n \\) is if \\( r = 1 \\). This is because if \\( r \\neq 1 \\), the magnitudes and phases do not generally match without specific values of \\( n \\).\n\nThus, \\( |z| = 1 \\).\n\n</think>\n<answer>\nThe magnitude of \\( z \\) is \\( |z| = 1 \\).\n\n</answer><|im_end|>\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}